{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Lembar Kerja] DSC UI Summer School 2020 - ML Basics",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wowothk/jupyter/blob/master/%5BLembar_Kerja%5D_DSC_UI_Summer_School_2020_ML_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldMXFMQh0x6u"
      },
      "source": [
        "# Dasar Pembelajaran Mesin / Machine Learning Basics\n",
        "\n",
        "**Bramantyo Adrian & Dimitrij Ray**\n",
        "\n",
        "**AI Engineer, GDP Labs**\n",
        "\n",
        "**Senin, 19 Oktober 2020**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ugWeIkh-lsv"
      },
      "source": [
        "# Problem\n",
        "Dataset yang akan digunakan adalah dataset `Adult` dari [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult). Dataset ini berisi sampel hasil sensus di Amerika Serikat yang diadakan tahun 1994. \n",
        "\n",
        "Kita akan menggunakan dataset ini untuk melatih yang dapat memprediksi apakah seseorang memiliki penghasilan lebih dari USD 50000 per tahun.\n",
        "\n",
        "Atribut di dalam data:\n",
        "- age: continuous.\n",
        "- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "- fnlwgt: continuous. (final weight)\n",
        "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "- education-num: continuous.\n",
        "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
        "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
        "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
        "- sex: Female, Male.\n",
        "- capital-gain: continuous.\n",
        "- capital-loss: continuous.\n",
        "- hours-per-week: continuous.\n",
        "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q71MAyJ9-pgv"
      },
      "source": [
        "**Sebelum Anda mulai:**\n",
        "\n",
        "Apabila Anda menggunakan Google Colaboratory, silakan jalankan sel berikut untuk membaharui beberapa *library* yang akan kita gunakan selama sesi ini.\n",
        "\n",
        "Jika Anda menggunakan Jupyter Notebook atau JupyterLab, silakan hapus sel ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKK3NKtY-qtQ"
      },
      "source": [
        "!pip install --upgrade numpy scipy pandas scikit-learn seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88sfmZgA1cRx"
      },
      "source": [
        "## Mengimpor *library*\n",
        "\n",
        "Untuk sesi praktik ini, kita akan menggunakan beberapa *library* yang umum digunakan untuk keperluan pembelajaran mesin, yaitu:\n",
        "1. `matplotlib` dan `seaborn` untuk visualisasi data,\n",
        "1. `numpy` dan `pandas` untuk penampungan dan pemrosesan data, serta\n",
        "1. `sklearn` untuk pemodelan.\n",
        "\n",
        "Jalankan sel berikut untuk mengimpor *library*-*library* tersebut. Kita akan terlebih dahulu mengimpor modul-modul yang dibutuhkan untuk pemrosesan data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN4QlOYcjb6r"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVl373bN3A9X"
      },
      "source": [
        "## Memuat dataset\n",
        "\n",
        "Lengkapi dan jalankan sel berikut untuk memuat dataset yang berformat `csv` ini ke dalam sebuah `DataFrame`. Perhatikan bahwa:\n",
        "\n",
        "1. Ada 2 macam berkas yang kita muat: `adult.data` yang berisi data latih dan `adult.test` yang berisi data uji.\n",
        "1. Masing-masing berkas `csv` **tidak** memiliki *header*, atau penunjuk nama-nama kolom di baris pertama. Oleh karena itu, melalui argumen `names` pada *method* `read_csv`, kita perlu memasukkan nama-nama kolomnya. \n",
        "  - Dalam situasi nyata, Anda harus mencocokkan nama kolom dengan deskripsi yang diberikan oleh pemilik data, tapi kali ini hal tersebut telah dilakukan untuk Anda.\n",
        "  - Dalam situasi nyata Anda juga mungkin menemukan berkas `csv` yang tidak terformat dnegan baik. Oleh karena itu, saat Anda menerima berkas, periksa terlebih dahulu menggunakan *text processor* favorit Anda (mis. Notepad++, Sublime Text)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXUXG1CVjdd3"
      },
      "source": [
        "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
        "\n",
        "data_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=col_names, delimiter=', ')\n",
        "\n",
        "## <--- LENGKAPI --->\n",
        "## Lengkapi sumber data untuk method read_csv, serta argumen names dan delimiter.\n",
        "## Kita akan membaca data uji dari https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
        "\n",
        "test_df = pd.read_csv(None, skiprows=1, names=None, delimiter=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKughQRR9JOg"
      },
      "source": [
        "Gunakan *method* `head(n)` untuk melihat $n$ baris pertama dari sebuah dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TwvyfLb9KUn"
      },
      "source": [
        "data_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34O6abWTXqN_"
      },
      "source": [
        "Kita bisa menggunakan *method* `notnull()` untuk mengetahui apakah ada baris-baris yang kosong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_OvPQXNWrL0"
      },
      "source": [
        "train_rows = len(data_df)\n",
        "train_without_missing = len(data_df.notnull())\n",
        "\n",
        "## <--- LENGKAPI --->\n",
        "## Hitunglah banyak baris di test_df, serta banyaknya baris yang tak-kosong.\n",
        "test_rows = None\n",
        "test_without_missing = None\n",
        "\n",
        "print(f'Train rows: {train_rows}; missing values: {train_rows - train_without_missing}')\n",
        "print(f'Test rows: {test_rows}; missing values: {test_rows - test_without_missing}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8YxlcXF7WnY"
      },
      "source": [
        "## Memisahkan data latih dan data validasi\n",
        "\n",
        "Dalam praktik pembelajaran mesin, apabila kita memiliki data latih yang cukup banyak, kita dapat menyisihkan sebagian data latih untuk digunakan sebagai data validasi.  Data validasi nanti akan kita gunakan untuk mengestimasi galat data uji.\n",
        "\n",
        "Biasanya pemisahan data latih, data validasi, dan data uji dilakukan dengan acak.  Akan tetapi, ada beberapa situasi yang memungkinkan dilakukannya cara lain untuk memisahkan dataset yang kita miliki.\n",
        "1. Apabila masalah yang akan kita selesaikan adalah masalah klasifikasi dan terjadi ketimpangan sampel data yang berasal dari kelas tertentu (situasi ini disebut *class imbalance*), kita bisa melakukan pemisahan menggunakan prinsip *stratified sampling* supaya proporsi masing-masing kelas terjaga untuk tiap-tiap partisi data.\n",
        "1. Apabila dataset dan masalah yang dihadapi berupa deret waktu (*time series*), kita dapat melakukan pemisahan berdasarkan satuan waktu tertentu (misalnya kita menggunakan data dari bulan 1 - 4 untuk melatih model dan bulan ke-6 untuk mengujinya)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAPo8NJcJI7z"
      },
      "source": [
        "Kita sekarang menghadapi masalah klasifikasi. Oleh karenanya, baik apabila kita melihat terlebih dahulu apakah kita menghadapi situasi *class imbalance* atau tidak.  Untuk itu, kita dapat menggunakan *method* `groupby()` dan `count()` untuk mengelompokkan data kita berdasarkan kelas dan menghitung banyaknya data yang ada di kelas tersebut."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-hNBkwXI5OK"
      },
      "source": [
        "data_df.groupby('class').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma7L_LDyKHIk"
      },
      "source": [
        "Dapat dilihat bahwa kita mayoritas orang dalam dataset berada di kelas pertama, yakni `<=50K`. Oleh karena itu, kita menggunakan teknik *stratified sampling* untuk membuat data validasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xf3_cU6lCgr"
      },
      "source": [
        "train_df, val_df = train_test_split(data_df, test_size=0.3, stratify=data_df['class'], random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZGyLT2K8JaK"
      },
      "source": [
        "## <--- LENGKAPI ---> \n",
        "## Gunakan .groupby() dan .class() untuk menghitung banyaknya baris di masing-\n",
        "## masing kelas di DataFrame train_df.  Bandingkan perbandingan antara kelas\n",
        "## '<=50K' dan '>50K' di data_df dan di train_df."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_CvrBXR8co3"
      },
      "source": [
        "## <--- LENGKAPI ---> \n",
        "## Lakukan hal yang sama untuk val_df."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiE28wSgKmm5"
      },
      "source": [
        "Mari kita lihat beberapa baris pertama data latih dan data validasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NfWX1Ti3Iey"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Cobalah melihat 5 baris pertama dari train_df.\n",
        "\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnKLfnJDVeTm"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Cobalah melihat 5 baris pertama dari val_df.\n",
        "\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdJ8zbcqSzWD"
      },
      "source": [
        "Sekilas, ada dua hal yang perlu diperhatikan:\n",
        "- Terdapat beberapa nilai `?`, yang sepertinya menyatakan nilai yang hilang (*missing values*)\n",
        "- Kelas target kita, variabel `class`, berbeda format di data uji.\n",
        "\n",
        "Masalah yang kedua dapat kita selesaikan dengan mudah saat melakukan *preprocessing*. Masalah yang pertama perlu kita teliti lebih lanjut. Mari kita lihat berapa baris untuk tiap kolom di data latih yang memiliki nilai `?` dan simpan kolom-kolom yang memiliki nilai `?`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h373M4B_ThNf"
      },
      "source": [
        "col_with_missing = list()\n",
        "for col in train_df.columns:\n",
        "  try:\n",
        "    missings = len(train_df[train_df[col] == '?'])\n",
        "    print(f'Column {col} has {missings} missing value(s)')\n",
        "    if missings != 0:\n",
        "      col_with_missing.append(col)\n",
        "  except ValueError:\n",
        "    print(f'Column {col} seems to have no missing values, since it is not of type string.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaLTcDoZYXtf"
      },
      "source": [
        "Terlihat bahwa beberapa kolom kategorikal memiliki nilai `?`.  Untuk variabel kategorikal, nilai kosong dapat dijadikan kategori baru atau diimputasi.  Untuk sesi ini, nanti kita akan memperlakukan nilai `?` sebagai kategori baru."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWiRTNQpzjoI"
      },
      "source": [
        "## Bagian 1: Analisis data eksploratif / *Exploratory data analysis* (EDA)\n",
        "\n",
        "Hal pertama yang harus dilakukan sebelum kita mulai melakukan transformasi dan pemodelan adalah menganalisis data yang ada.\n",
        "\n",
        "Pada bagian ini kita hanya akan melihat sedikit saja eksplorasi yang bisa dilakukan terhadap dataset ini. Anda dipersilakan untuk mengeksplorasi sendiri setelah sesi ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaYu184BwHQZ"
      },
      "source": [
        "### Analisis atribut\n",
        "\n",
        "Pertama, kita dapat melihat kolom apa saja yang tersedia beserta tipenya (apakah akan kita perlakukan sebagai variabel kontinu atau kategorikal). Selain itu, apabila tersedia, kita juga dapat mencocokkan nilai-nilai setiap kolom dengan spesifikasi dataset tersebut."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ERniS5T2B79"
      },
      "source": [
        "train_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWBT4Tcua2A5"
      },
      "source": [
        "train_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cltjaDCYZtRU"
      },
      "source": [
        "`DataFrame` `pandas` memiliki *method*\n",
        " `describe()` yang dapat membantu menghitung statistik deskriptif dari suatu dataset. Untuk kolom-kolom yang diperlakukan sebagai variabel kontinu, `describe()` mengeluarkan mean, standar deviasi, nilai minimum dan maksimum, serta nilai kuartil.\n",
        "\n",
        "Untuk membuang `continuous_columns` dan `ordinal_columns` dari `train_df.columns`, kita dapat memanfaatkan tipe data `set` dari Python. Operasi `set(a) - set(b)` akan mengeluarkan elemen-elemen yang ada di `a` tetapi tidak di `b`.\n",
        "\n",
        "Untuk memilih kolom-kolom tertentu saja dari suatu `DataFrame`, kita dapat menggunakan `df[['a', 'b', 'c', 'd']]` untuk memilih kolom `a`, `b`, `c`, `d` di `DataFrame` `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdlnSO1w60EE"
      },
      "source": [
        "continuous_columns = ['capital-gain', 'capital-loss', 'hours-per-week', 'age', 'fnlwgt']\n",
        "ordinal_columns = ['education', 'education-num']\n",
        "label_col = 'class'\n",
        "categorical_columns = sorted(list(set(train_df.columns) - set(continuous_columns + ordinal_columns + [label_col])))\n",
        "\n",
        "continuous_df = train_df[continuous_columns]\n",
        "continuous_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZFe_5wKanlt"
      },
      "source": [
        "Untuk kolom-kolom yang bertipe `object`, `describe()` akan menghitung banyaknya nilai yang unik dan nilai yang sering muncul beserta frekuensinya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUup2ua39w6P"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Hitunglah statistik deskriptif label (variabel class) dan kolom yang termasuk \n",
        "## dalam list categorical_columns di data latih.\n",
        "\n",
        "categorical_df = None\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llJdQx5ZgFLc"
      },
      "source": [
        "Kita dapat menggunakan kelas `Categorical` dari `pandas` untuk mengubah suatu kolom yang tadinya bertipe `int` menjadi tipe `category`.  Metode `describe()` akan mengeluarkan keluaran yang sama seperti ketika kita menggunakannya untuk kolom yang bertipe `object`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qikyB6mpAuJY"
      },
      "source": [
        "ordinal_data = train_df[ordinal_columns]\n",
        "ordinal_data.loc[:, ['education-num']] = pd.Categorical(ordinal_data['education-num'])\n",
        "print(ordinal_data.dtypes)\n",
        "\n",
        "ordinal_data[ordinal_columns].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smry54LELZCn"
      },
      "source": [
        "### Analisis univariat\n",
        "\n",
        "Dari analisis sederhana tadi, ada setidaknya tiga hal yang menarik perhatian:\n",
        "1. Standar deviasi `capital-gain` tinggi sekali, mencapai 7552, padahal kuartil-kuartilnya 0.\n",
        "1. Variabel `education-num` dan `education` memiliki nilai paling sering dengan frekuensi yang sama.\n",
        "1. Terdapat banyak variabel kontinu yang berbeda satuan/skala.\n",
        "\n",
        "Temuan pertama dapat kita lihat lebih jauh menggunakan analisis univariat. Temuan kedua dapat kita telusuri lebih jauh menggunakan analisis bivariat. Temuan ketiga langsung dapat kita tindak lanjuti saat melakukan rekayasa fitur dengan cara melakukan *scaling*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgtNFqIiXp0_"
      },
      "source": [
        "Mari kita mulai dengan menginvestigasi `capital-gain`. Kita dapat menggunakan *method* `hist(column: list, figsize: tuple, layout: tuple, bins: int)` di suatu `DataFrame` untuk membuat histogram secara otomatis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dZ5DK7ABQSh"
      },
      "source": [
        "train_df.hist(column=\"capital-gain\", figsize=(10,30), layout=(5,1), bins=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yapAfA8ZGgkM"
      },
      "source": [
        "Terlihat bahwa sepertinya sebagian besar nilai variabel `capital-gain` berada di bawah 40000, akan tetapi ada sebagian kecil yang memiliki nilai di atas 90000.  Sepertinya kita berhadapan dengan pencilan. Mari kita lihat beberapa sampel yang memiliki nilai lebih besar dari atau sama dengan 90000.  Kita dapat menggunakan *method* `sample(n)` untuk mengambil *n* buah sampel tanpa pengembalian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNyFtAskG3Z7"
      },
      "source": [
        "train_df_high_capital = train_df[train_df['capital-gain'] >= 90000]\n",
        "\n",
        "train_df_high_capital.sample(10, random_state=21).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSn42lFjIwyN"
      },
      "source": [
        "Kita juga dapat menggunakan *method* `len` untuk menghitung banyaknya baris di suatu `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHm8jAntIg7A"
      },
      "source": [
        "print(len(train_df_high_capital))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1S0GLaAI1G5"
      },
      "source": [
        "Dari sampel, terlihat bahwa orang-orang dengan `capital-gain` 99999 ini adalah orang-orang dengan posisi karier yang cukup tinggi, misalnya guru besar atau bagian eksekutif-manajerial.  Nilai 99999 sepertinya adalah nilai *cap* yang ditetapkan oleh badan sensus ketika `capital-gain` seseorang bernilai 100000 atau lebih.\n",
        "\n",
        "Lebih dari itu, orang-orang ini sepertinya hampir pasti memiliki penghasilan di atas USD 50000. Oleh karena itu, sebaiknya pencilan-pencilan ini tidak dibuang, akan tetapi kita kontrol dengan cara melakukan *binning*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7id8ax7vVVX7"
      },
      "source": [
        "## Analisis bivariat\n",
        "\n",
        "Berikutnya, kita dapat menginvestigasi `education` dan `education-num`. Untuk mencari tahu apakah benar `education-num` dan `education` memiliki korespondensi satu-satu, kita dapat menggunakan tabel pivot dan fitur *heatmap* dari `seaborn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUM-X0fSMdPn"
      },
      "source": [
        "train_df_dummy = train_df.copy()\n",
        "train_df_dummy['dummy'] = 1\n",
        "edu_pivot = pd.pivot_table(train_df_dummy, \n",
        "                           values='dummy', \n",
        "                           index='education-num', \n",
        "                           columns='education', \n",
        "                           aggfunc='count')\n",
        "\n",
        "edu_pivot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8W4CRdSShvv"
      },
      "source": [
        "Terlihat bahwa banyak sekali entri yang kosong, ditandai dengan `NaN` (*not a number*). Kita akan biarkan entri-entri ini kosong, sebab `seaborn` secara otomatis akan mengosongkan bagian tersebut juga saat membuat *heatmap*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hytCHF7wOnMW"
      },
      "source": [
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(edu_pivot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0yNIo08S0mS"
      },
      "source": [
        "Terlihat dari *heatmap* bahwa memang setiap entri di `education-num` dan `education` saling bersesuaian. Oleh karena itu, nantinya ketika melakukan rekayasa fitur kita cukup menggunakan satu kolom saja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAc-W_-3katJ"
      },
      "source": [
        "Ada pula beberapa jenis analisis bivariat lain yang dapat dilakukan. Sebagai contoh, kita ingin tahu apakah variabel `fnlwgt` memiliki pengaruh terhadap `class`.\n",
        "\n",
        "Hal pertama yang bisa lakukan adalah menggambar *box plot*, yang akan menunjukkan kuartil-kuartil `fnlwgt` berdasarkan `class`, serta pencilan yang dihitung menggunakan jarak antarkuartil (*interquartile range*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv9dq-1BVUsP"
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "sns.boxplot(x='class', y='fnlwgt', data=train_df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMa3xaA6dnJh"
      },
      "source": [
        "Dari *box plot*, terlihat bahwa sepertinya kuartil-kuartil `fnlwgt` mirip sekali di kedua kelas. Keduanya pun sepertinya merupakan distribusi yang menceng ke kanan (*right-skewed*). \n",
        "\n",
        "Oleh karenanya, kita mungkin dapat mengasumsikan bahwa variabel `fnlwgt` mungkin tidak dapat menjadi pembeda yang baik antara kedua kelas. Untuk mengecek apakah kita *mungkin* ingin berubah pikiran, kita dapat melakukan uji hipotesis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQlm0KhZ4AFE"
      },
      "source": [
        "train_df[train_df['class'] == '>50K']['fnlwgt'].hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPTX7lyY4B0L"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Periksalah histogram fnlwgt untuk kelas '<=50K'.\n",
        "\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-SxeYAena1"
      },
      "source": [
        "Kita akan menggunakan Welch's $t$-test, yakni variasi uji-$t$ yang memiliki asumsi:\n",
        "- kedua populasi berdistribusi normal,\n",
        "- kedua populasi mungkin memiliki variansi yang tidak sama.\n",
        "\n",
        "Karena distribusi yang kita miliki menceng ke kanan, kita dapat melakukan transformasi logaritma terhadap nilai `fnlwgt` sehingga kita mendapatkan distribusi yang lebih simetris. Kita asumsikan distribusi `log(fnlwgt)` ini adalah normal. Lengkapi dan jalankan sel-sel berikut untuk melihat histogram `log(fnlwgt)` untuk kedua kelas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O2MfQnecKzs"
      },
      "source": [
        "np.log(train_df[train_df['class'] == '>50K']['fnlwgt']).hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrUHGhhlcUtT"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Buatlah histogram hasil log(fnlwgt) untuk kelas '<=50K' yang memiliki 20 bin.\n",
        "\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YLAIPDIgA3N"
      },
      "source": [
        "Kita akan menggunakan modul `stats` *library* `scipy`.   \n",
        "\n",
        "Hipotesis-hipotesis kita adalah sebagai berikut.\n",
        "\n",
        "$H_0$: Mean dari `fnlwgt` di kedua kelas sama.\n",
        "\n",
        "$H_1$: Mean dari `fnlwgt` di kedua kelas tidak sama.\n",
        "\n",
        "Kita akan menggunakan taraf signifikansi $\\alpha = 0.05$. \n",
        "\n",
        "Jalankan sel berikut untuk mengimpor *method* `ttest_ind` `scipy.stats` dan menjalankan uji-$t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvzo6fnifbOY"
      },
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "positive = np.log(train_df[train_df['class'] == '>50K']['fnlwgt'])\n",
        "negative = np.log(train_df[train_df['class'] == '<=50K']['fnlwgt'])\n",
        "\n",
        "test_stat, pval = ttest_ind(positive, negative, equal_var = False)\n",
        "\n",
        "print(\"Test statistic (T) :\", test_stat)\n",
        "print(\"P-value: \", pval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFyLuMWfgLdu"
      },
      "source": [
        "Kita melihat *p-value* yang lebih tinggi daripada taraf signifikansi yang telah ditetapkan di awal. Oleh karena itu, kita memutuskan bahwa hipotesis awal kita tidak ditolak.  Ini berarti: data yang kita miliki tidak cukup meyakinkan kita untuk menolak asumsi awal kita bahwa mean dari `fnlwgt` di kedua kelas adalah sama.\n",
        "\n",
        "Oleh karena itu, karena kita menginginkan model yang lebih sederhana, kita tidak akan menggunakan variabel `fnlwgt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzYzAHdVpyud"
      },
      "source": [
        "# Bagian 2: *Preprocessing* dan rekayasa fitur (*feature engineering*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdGBrLSQ5JXL"
      },
      "source": [
        "Berdasarkan hasil analisis data, kita dapat melakukan tahap selanjutnya, yaitu ***preprocessing* & rekayasa fitur**. Kita akan:\n",
        "\n",
        "1. Membuang kolom yang tidak akan digunakan,\n",
        "1. Menyederhanakan nilai-nilai atribut kategorikal,\n",
        "1. Melakukan *one-hot encoding* untuk atribut kategorikal,\n",
        "1. Normalisasi, dan\n",
        "1. *Binning* atribut numerik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McSxk0ZVqZt3"
      },
      "source": [
        "### Membuang kolom yang tidak digunakan\n",
        "\n",
        "Dari hasil analisis data, kita memutuskan:\n",
        "- Akan menggunakan salah satu saja dari kolom `education` dan `education-num`; dalam kasus ini kita akan mengambil kolom `education-num` saja karena memiliki informasi urutan.\n",
        "- Tidak akan menggunakan `fnlwgt`.\n",
        "\n",
        "Oleh karena itu, kita akan membuang kolom `fnlwgt` dan `education`. Kita dapat menggunakan *method* `drop(columns: list)` untuk melakukan ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkg_gerGqlHv"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Berdasarkan deskripsi di atas, lengkapi variabel dropped_cols dan buang \n",
        "## dropped_cols dari train_df.\n",
        "\n",
        "dropped_cols = None\n",
        "prep_train_df = train_df.drop(columns=None)\n",
        "prep_train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lb_p_cwcyJ"
      },
      "source": [
        "### Menyederhanakan nilai-nilai atribut kategorikal\n",
        "\n",
        "Kebanyakan modul pembuatan model hanya akan menerima masukan label dalam bentuk numerik, umumnya 0 dan 1. Oleh karena itu, kita perlu mengubah label di variabel `class` menjadi nilai 0 dan 1.\n",
        "\n",
        "Selain itu, perhatikan bahwa kita memiliki beberapa atribut kategorikal yang memiliki nilai cukup banyak, misalnya `native-country` dan `marital-status`. Karena nanti kita akan menggunakan metode *one-hot encoding*, variasi nilai yang banyak akan mengakibatkan pembuatan kolom baru yang juga banyak, sehingga menambah kompleksitas model.  Oleh karena itu, kita akan menyederhanakan kedua variabel tersebut dengan cara memetakan nilai yang lama ke nilai yang baru.\n",
        "\n",
        "Kedua masalah ini dapat diselesaikan dengan *method* `map(mapper: dict)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km6EpG0Cwbvz"
      },
      "source": [
        "def map_categorical_values(df: DataFrame, col_map_pairs: [(str, dict)]):\n",
        "    result_df = df.copy()\n",
        "    for col, map_ in col_map_pairs:\n",
        "      result_df[col] = result_df[col].map(map_)\n",
        "      \n",
        "    return result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX9p2foGS1mA"
      },
      "source": [
        "def map_label(df: DataFrame, label_col: str, positive_label: str, negative_label: str):\n",
        "  result_df = df.copy()\n",
        "  result_df[label_col] = result_df[label_col].map({positive_label: 1, negative_label: 0})\n",
        "\n",
        "  return result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecwfRB-DxMoz"
      },
      "source": [
        "col_map_pairs = [\n",
        "    (\n",
        "        'marital-status', {\n",
        "            'Married-civ-spouse': 'Couple', \n",
        "            'Divorced': 'Single',\n",
        "            'Never-married': 'Single',\n",
        "            'Separated': 'Single', \n",
        "            'Widowed': 'Single',\n",
        "            'Married-spouse-absent': 'Single',\n",
        "            'Married-AF-spouse': 'Couple'\n",
        "        }\n",
        "    ),\n",
        "    (\n",
        "        'native-country', {\n",
        "          'United-States': 'United-States', \n",
        "          'Cambodia': 'Southeast-Asia', \n",
        "          'England': 'Europe', \n",
        "          'Puerto-Rico': 'America', \n",
        "          'Canada': 'Canada', \n",
        "          'Germany': 'Europe',\n",
        "          'Outlying-US(Guam-USVI-etc)': 'America', \n",
        "          'India': 'Asia', \n",
        "          'Japan': 'Asia', \n",
        "          'Greece': 'Europe', \n",
        "          'South': 'Asia', \n",
        "          'China': 'Asia', \n",
        "          'Cuba': 'America', \n",
        "          'Iran': 'Asia', \n",
        "          'Honduras': 'America', \n",
        "          'Philippines': 'Southeast-Asia', \n",
        "          'Italy': 'Europe', \n",
        "          'Poland': 'Europe', \n",
        "          'Jamaica': 'America', \n",
        "          'Vietnam': 'Southeast-Asia', \n",
        "          'Mexico': 'America', \n",
        "          'Portugal': 'Europe', \n",
        "          'Ireland': 'Europe', \n",
        "          'France': 'Europe', \n",
        "          'Dominican-Republic': 'America', \n",
        "          'Laos': 'Southeast-Asia', \n",
        "          'Ecuador': 'America', \n",
        "          'Taiwan': 'Asia', \n",
        "          'Haiti': 'America', \n",
        "          'Columbia': 'America', \n",
        "          'Hungary': 'Europe',\n",
        "          'Guatemala': 'America', \n",
        "          'Nicaragua': 'America', \n",
        "          'Scotland': 'Europe', \n",
        "          'Thailand': 'Southeast-Asia', \n",
        "          'Yugoslavia': 'Europe', \n",
        "          'El-Salvador': 'America', \n",
        "          'Trinadad&Tobago': 'America', \n",
        "          'Peru': 'America', \n",
        "          'Hong': 'Asia', \n",
        "          'Holand-Netherlands': 'Europe',\n",
        "          '?': '?'\n",
        "      }\n",
        "    )\n",
        "]\n",
        "prep_train_df_label_mapped = map_label(prep_train_df, 'class', '>50K', '<=50K')\n",
        "prep_train_df_simplified = map_categorical_values(prep_train_df_label_mapped, col_map_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmwdkqmlya42"
      },
      "source": [
        "for col, _ in col_map_pairs:\n",
        "  print(\"Unique values of {}: {}\".format(col, prep_train_df_simplified[col].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1EfoJhWHzsJ"
      },
      "source": [
        "### *One-hot encoding* untuk atribut kategorikal\n",
        "\n",
        "*One-hot encoding* adalah salah satu cara yang kerap dipakai untuk memanipulasi fitur-fitur kategorikal agar bisa dimasukkan ke dalam sebuah model yang umumnya berupa suatu persamaan. \n",
        "\n",
        "Misalkan variabel `buah` memiliki nilai `['apel', 'belimbing', 'cempedak', 'unknown']`. Masing-masing nilai di variabel tersebut dapat dinyatakan sebagai vektor $v \\in \\{0, 1\\}^4$ sebagai berikut:\n",
        "- `apel`: $(1, 0, 0, 0)$\n",
        "- `belimbing`: $(0, 1, 0, 0)$\n",
        "- `cempedak`: $(0, 0, 1, 0)$\n",
        "- `unknown`:  $(0, 0, 0, 1)$\n",
        "\n",
        "Alternatifnya, kita dapat menyatakan masing-masing nilai sebagai vektor di $\\{0, 1\\}^3$ dengan memerlakukan (misalnya) `unknown` sebagai kategori \"lain-lain\", sehingga bentuk *encoding*-nya adalah:\n",
        "\n",
        "- `apel`: $(1, 0, 0)$\n",
        "- `belimbing`: $(0, 1, 0)$\n",
        "- `cempedak`: $(0, 0, 1)$\n",
        "- `unknown`: $(0, 0, 0)$\n",
        "\n",
        "Kita akan menggunakan konvensi yang *kedua*: jika variabel kategorikal kita memiliki $d$ variasi, maka kita akan mengkodekan variabel tersebut sebagai vektor di $\\{0, 1\\}^{d-1}$.\n",
        "\n",
        "Kita akan menggunakan kelas `OneHotEncoder` dari `scikit-learn` untuk men-*encode* kolom-kolom di `categorical_columns`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MXwGVGwLPal"
      },
      "source": [
        "print(categorical_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKlROtl0A93g"
      },
      "source": [
        "Sebenarnya, karena kita sudah mengetahui kemungkinan isi dari masing-masing kolom berdasarkan deskripsi, kita bisa saja mendefinisikan kategori yang akan dikenali oleh `OneHotEncoder`. Perhatikan bahwa urutan memasukkan kategori untuk setiap kolom *harus* sesuai dengan urutan kolom yang akan di-*encode*.\n",
        "\n",
        "Dalam praktiknya hal ini tidak selalu terjadi. Apabila kita tidak mengetahui kemungkinan isi dari masing-masing kolom, **tidak perlu** menggunakan argumen `categories`. Cukup panggil metode `fit`, dan `OneHotEncoder` akan secara otomatis belajar dari data latih yang Anda masukkan.\n",
        "\n",
        "Untuk sesi ini, kita tidak akan mendefinisikan `categories`. \n",
        "\n",
        "Jalankan sel berikut untuk melihat hasil *one-hot encoding* kolom-kolom `categorical_columns`. Perhatikan bahwa ada dua *method* yang harus dipanggil: `fit()` untuk \"melatih\" `OneHotEncoder` mengenali kategori-kategori di dataset, dan `transform()` untuk melakukan *encoding*. Metode `fit`-`transform` ini akan sering kita lihat pada saat melakukan *preprocessing*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5xDefJbA1QP"
      },
      "source": [
        "ohe_encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "ohe_encoder.fit(prep_train_df_simplified[categorical_columns])\n",
        "ohe_encoder.transform(prep_train_df_simplified[categorical_columns])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yPWkKO-kRfL"
      },
      "source": [
        "### *Scaling* atribut kontinu\n",
        "\n",
        "Salah satu penemuan kita pada saat analisis data adalah fakta bahwa banyak variabel kontinu yang memiliki skala berbeda. Agar model kita tidak terpengaruh terhadap skala, kita dapat melakukan *scaling*. Ada dua metode *scaling* sederhana yang kerap digunakan. Misalkan kita memiliki nilai $x$ di suatu variabel $X$:\n",
        "1. *Min-max scaling*: mengurangi nilai $x$ dengan nilai minimum variabel $X$ di dataset, kemudian membaginya dengan jangkauan (*range*) variabel tersebut:\n",
        "$$x^{(\\text{scaled})} = \\frac{x - X_\\text{min}}{X_\\text{max} - X_\\text{min}}.$$\n",
        "Dapat ditunjukkan bahwa metode *scaling* ini akan menghasilkan variabel yang berada pada rentang $[0, 1]$.\n",
        "1. *Standardization* atau *standard scaling*: mengurangi nilai $x$ dengan (estimasi) mean variabel $X$, kemudian membaginya dengan (estimasi) standar deviasinya:\n",
        "$$ x^{(\\text{scaled})} = \\frac{x - \\mu_X}{\\sigma_X}.\n",
        "$$\n",
        "Dapat ditunjukkan bahwa metode *scaling* ini akan menghasilkan variabel yang memiliki mean $0$ dan variansi $1$.\n",
        "\n",
        "Metode *scaling* yang digunakan bergantung pada masalah dan dataset yang dihadapi. Untuk sesi ini, kita akan menggunakan *standard scaler*.\n",
        "\n",
        "`scikit-learn` memiliki kelas `StandardScaler` yang dapat digunakan untuk keperluan ini. Untuk menggunakannya, pertama kita harus memanggil *method* `fit()` supaya objek *scaler* kita tahu mean dan standar deviasi kolom yang ingin kita transformasikan. Setelah itu, kita memanggil *method* `transform` untuk melakukan transformasinya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xC42tW0lWd_"
      },
      "source": [
        "standardized_cols = ['age', 'hours-per-week']\n",
        "scaler = StandardScaler()\n",
        "\n",
        "## <--- LENGKAPI --->\n",
        "## \"Latih\"lah scaler menggunakan kolom-kolom standardized_cols dari \n",
        "## prep_train_df_simplified, kemudian transformasikan kolom-kolom yang sama \n",
        "## menggunakan scaler yang telah di\"latih\".\n",
        "\n",
        "None\n",
        "\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Njx6IJB4Cl"
      },
      "source": [
        "Perhatikan bahwa `OneHotEncoder` dan `StandardScaler` hanya melakukan transformasi terhadap kolom-kolom yang dipilih saja, tetapi tidak mendukung transformasi satu DataFrame. Kita akan lihat nanti bagaimana cara mengatasi ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AceofSdnTiB"
      },
      "source": [
        "### *Binning* atribut kontinu\n",
        "\n",
        "Hal lain yang dapat dilakukan terhadap variabel kontinu adalah melakukan *binning* atau pengelompokan. \n",
        "\n",
        "Ada bermacam-macam kriteria pengelompokan. Dua di antaranya yang paling sederhana adalah:\n",
        "1. *Uniform intervals*, yakni pengelompokkan yang dilakukan sedemikian rupa sehingga panjang interval masing-masing kelompok sama.\n",
        "2. *Quantile-based*, yakni pengelompokkan yang dilakukan sedemikian rupa sehingga masing-masing interval memiliki banyak sampel yang sama.\n",
        "\n",
        "Kita juga bisa menentukan batas-batas interval setiap kelompok berdasarkan pengetahuan sebelumnya atau berdasarkan hasil analisis data.\n",
        "\n",
        "`scikit-learn` memiliki kelas `KBinsDiscretizer` untuk melakukan *binning*. Hanya saja, kelas tersebut tidak mendukung pembuatan *bin* yang didefinisikan sendiri. Untuk keperluan sesi ini, Anda telah dibuatkan fungsi `bin_numerical_attributes` yang dapat menerima *bin* yang didefinisikan sendiri.  Kita akan menggunakan fungsi ini untuk mengelompokkan `capital-gain` dan `capital-loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUUYTrhxpsCx"
      },
      "source": [
        "def _map_value_to_bin(value: int, bin_thresholds: [float]):\n",
        "  count = 0\n",
        "  \n",
        "  for threshold in bin_thresholds:\n",
        "    if value < threshold:\n",
        "      return count\n",
        "    else:\n",
        "      count += 1\n",
        "    \n",
        "\n",
        "def bin_numerical_attributes(df: DataFrame, col_bin_pairs: [(str, [float])] ):\n",
        "    result_df = df.copy()\n",
        "    for col, bin_thresholds in col_bin_pairs:\n",
        "      result_df[col] = result_df[col].apply(lambda val: _map_value_to_bin(val, bin_thresholds))\n",
        "    \n",
        "    return result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YibGHu8MhAXP"
      },
      "source": [
        "bin_capital_gain = [0.0, 1.0, 3000.0, 5000.0, 10000.0, 20000.0, float('inf')] \n",
        "bin_capital_loss = [0.0, 1.0, 1700, 1900.0, 2000, float('inf')]\n",
        "\n",
        "col_bin_pairs = [('capital-gain', bin_capital_gain), ('capital-loss', bin_capital_loss)]\n",
        "\n",
        "prep_train_df_binned = bin_numerical_attributes(prep_train_df_scaled, col_bin_pairs)\n",
        "\n",
        "prep_train_df_binned.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv7kfU6kCDA8"
      },
      "source": [
        "### Menggunakan `ColumnTransformer`\n",
        "\n",
        "Untuk menyatukan transformasi yang menggunakan kelas `scikit-learn` pada kolom berbeda-beda, kita dapat menggunakan kelas `ColumnTransformer`. Mari kita gunakan kelas ini untuk menyatukan `OneHotEncoder` dan `StandardScaler`.  Sebagai contoh, kita tetap akan menstandardisasi kolom-kolom di `standardized_cols`, tetapi kita hanya akan meng-`encode` kolom `sex`.  \n",
        "\n",
        "Jalankan sel berikut, dan bandingkan hasilnya dengan baris ke-2 *one-hot encoding*, standardisasi, dan *binning* yang telah Anda lakukan sebelumnya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HGbQuaGCD-Z"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "ct_example = ColumnTransformer(\n",
        "    [\n",
        "     ('standar-scaler-1', StandardScaler(), standardized_cols),\n",
        "     ('ohe-1', OneHotEncoder(drop=None, sparse=False), ['sex'])\n",
        "    ],\n",
        "    remainder='passthrough',\n",
        "    verbose=True\n",
        "     )\n",
        "\n",
        "ct_example.fit(bin_df)\n",
        "\n",
        "ct_example.transform(bin_df)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw4MKkEyzW6g"
      },
      "source": [
        "## Bagian 3: Melatih model\n",
        "\n",
        "Setelah kita selesai melakukan pemrosesan, saatnya kita mulai melatih model. Biasanya, dalam proses pembuatan model, kita akan memulai dengan model yang sederhana dan dengan rekayasa fitur yang minimum. Model sederhana yang pertama kali kita buat ini kita sebut sebagai model tolok-ukur atau *benchmark*. \n",
        "\n",
        "Setiap model berikutnya yang akan kita buat sebaiknya memiliki performa yang lebih baik secara signifikan dari model *benchmark*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rB0s1LCKlsF"
      },
      "source": [
        "### Model *benchmark*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGovZJMKeRq"
      },
      "source": [
        "Kita akan menggunakan beberapa model sederhana sebagai model *benchmark*. Perbedaan perlakuan kita kepada model-model tersebut nanti hanya pada bagian bagaimana kita merekayasa fitur. Hal ini kita lakukan untuk melihat pengaruh rekayasa fitur terhadap performa model. Untuk itu, kita akan membuat beberapa fungsi bantuan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p89NahOCCYaf"
      },
      "source": [
        "Kebanyakan *library* memerlukan fitur dan label sebagai masukan terpisah. Oleh karena itu, kita perlu memisahkan fitur dan labelnya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCRQM702CcPT"
      },
      "source": [
        "def separate_data_and_label(df: DataFrame, label_col: str):\n",
        "  ## <--- LENGKAPI --->\n",
        "  ## Pertama, buanglah label_col dari list kolom-kolom di df. Simpan sisanya\n",
        "  ## di dalam features.\n",
        "  features = None\n",
        "\n",
        "  ## Kemudian, ambil kolom-kolom di features dan simpan di data_df. \n",
        "  ## Terakhir, ambil kolom label_col dan simpan di label.\n",
        "  data_df = None\n",
        "  label = None\n",
        "  \n",
        "  return data_df, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4_gfKLMDEuC"
      },
      "source": [
        "Kemudian, kita buat terlebih dahulu model `ColumnTransformer` untuk *one-hot encoding*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw5pnk-0DFyP"
      },
      "source": [
        "bm_column_tf = ColumnTransformer(\n",
        "    [\n",
        "     ('ohe-bm-1', OneHotEncoder(drop='first', sparse=False), categorical_columns)\n",
        "    ],\n",
        "    remainder='passthrough',\n",
        "    verbose=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4lJkUeQDPcY"
      },
      "source": [
        "Kita sudah dapat mulai melakukan pemrosesan. Secara umum, kita akan \n",
        "melakukan transformasi-transformasi yang membutuhkan informasi kolom `pandas` terlebih dahulu, diikuti dengan transformasi yang dilakukan oleh `scikit-learn`.\n",
        "\n",
        "Pertama, mari kita proses data latih terlebih dahulu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzz9UZpDRVx"
      },
      "source": [
        "bm_train_raw = train_df.drop(columns=dropped_cols)\n",
        "bm_train_map = map_categorical_values(bm_train_raw, col_map_pairs)\n",
        "bm_train_label_map = map_label(bm_train_map, label_col, '>50K', '<=50K')\n",
        "bm_train_df, bm_train_label = separate_data_and_label(bm_train_label_map, label_col)\n",
        "\n",
        "bm_column_tf.fit(bm_train_df)\n",
        "\n",
        "bm_train_df_final = bm_column_tf.transform(bm_train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeSON2qjDVvg"
      },
      "source": [
        "Lalu kita proses data validasi. Perhatikan bahwa kita tidak perlu memanggil `fit` lagi untuk `bm_column_tf`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcm3-2pbDUcT"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Proseslah val_df dengan langkah-langkah seperti yang dilakukan terhadap\n",
        "## train_df.  Anda tidak perlu memanggil bm_column_tf.fit() lagi.\n",
        "\n",
        "bm_val_raw = None\n",
        "bm_val_map = None\n",
        "bm_val_label_map = None\n",
        "bm_val_df, bm_val_label = None\n",
        "\n",
        "bm_val_df_final = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfJnV2DCDpky"
      },
      "source": [
        "Karena `bm_train_df_final` dan `bm_val_df_final` bukan berupa `DataFrame` lagi, maka kita tidak bisa menggunakan `head()`. Kita cukup mengambil baris pertama saja dan menggunakan `print()` untuk mencetak."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPSHNw-SDruu"
      },
      "source": [
        "print(bm_train_df_final[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0HLo66E9I4T"
      },
      "source": [
        "Saatnya memodelkan! Jalankan sel-sel berikut ini untuk memuat beberapa model beserta metrik performa model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdKsZC-V2Ok8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZq5cdSWXnUx"
      },
      "source": [
        "Kita akan mencoba memodelkan menggunakan model regresi logistik. Ingat bahwa kita memiliki `(bm_train_df, bm_train_label)`, fitur dan label data uji dan\n",
        "`(bm_val_df, bm_val_label)`, fitur dan label data validasi.\n",
        "\n",
        "Pertama, jalankan sel berikut untuk membuat objek `LogisticRegression`. Kita hanya akan memberikan argumen *constructor* berupa `max_iter`, yakni maksimum banyaknya iterasi. Anda dapat membuka [dokumentasi](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) `sklearn` untuk mengetahui hal-hal apa saja yang bisa diubah.\n",
        "\n",
        "Pastikan bahwa konfigurasi model saat membuat objek sudah sesuai dengan yang Anda inginkan, misalnya:\n",
        "- Jenis penalti regularisasi sudah sesuai (mis. `l2` atau `l1`).\n",
        "- *Random state* sudah diisi apabila Anda ingin hasil Anda bisa direproduksi.\n",
        "- Maksimum iterasi sudah sesuai."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2LV93SyYB6q"
      },
      "source": [
        "lr_model = LogisticRegression(max_iter=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmi5QNvdZXff"
      },
      "source": [
        "Berikutnya, untuk melatih model, kita dapat menggunakan metode `fit(X, y)`, dengan `X` adalah fitur-fitur di data latih dan `y` adalah label-labelnya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHKoihlJZW1H"
      },
      "source": [
        "lr_model.fit(bm_train_df_final, bm_train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yao7RfdZjSZ"
      },
      "source": [
        "Terakhir, untuk membuat prediksi terhadap data validasi, kita dapat menggunakan metode `predict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdhtSE_maj1U"
      },
      "source": [
        "lr_model_predictions = lr_model.predict(bm_val_df_final)\n",
        "\n",
        "print(lr_model_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9maTPpbled"
      },
      "source": [
        "Karena kita memiliki label data uji, kita dapat melakukan evaluasi. Sebagai contoh, kita dapat menggunakan *method* `accuracy_score`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OetOCsSgcEiP"
      },
      "source": [
        "accuracy_score(bm_val_label, lr_model_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUnBnDZpcL_B"
      },
      "source": [
        "Daftar lengkap metrik performa yang dapat digunakan dapat Anda lihat di [dokumentasi](https://scikit-learn.org/stable/modules/classes.html?highlight=metrics#sklearn-metrics-metrics) modul `sklearn.metrics`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3VKsNsUR7ul"
      },
      "source": [
        "Di sel-sel berikut, kita akan membuat beberapa fungsi bantuan untuk melakukan evaluasi.\n",
        "\n",
        "Pertama, `evaluate_prediction` akan menerima label dan hasil prediksi dari suatu model, kemudian menghitung dan mencetak hasil prediksi model tersebut."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV5jBYKBqiJ8"
      },
      "source": [
        "def evaluate_prediction(label: pd.Series, prediction: pd.Series):\n",
        "  conf_matrix = confusion_matrix(label, prediction)\n",
        "  acc = accuracy_score(label, prediction)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(label, prediction, average='binary')\n",
        "\n",
        "  eval_result_template = f\"\"\"\n",
        "  Confusion Matrix (True Class VS Predicted Class)\n",
        "  {conf_matrix}\n",
        "\n",
        "  Accuracy  : {acc}\n",
        "  Precision : {precision}\n",
        "  Recall    : {recall}\n",
        "  F1-Score  : {f1}\n",
        "  \"\"\"\n",
        "  \n",
        "  print(eval_result_template)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zMqpZTdUpBX"
      },
      "source": [
        "Kemudian, `evaluate_model` akan menerima:\n",
        "1. nama model,\n",
        "1. objek model yang akan dilatih,\n",
        "1. data latih tanpa label,\n",
        "1. label data latih,\n",
        "1. data uji tanpa label, dan\n",
        "1. label data uji.\n",
        "\n",
        "Fungsi `evaluate_model` ini kemudian akan melatih model menggunakan *method* `fit` dan melakukan prediksi dengan *method* `predict`. Setelah itu, hasil prediksinya akan dimasukkan ke dalam `evaluate_prediction` untuk dihitung performanya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiCn5KzWbX3L"
      },
      "source": [
        "def evaluate_model(model_name: str, model: object, train_df: np.ndarray, train_label: pd.Series, test_df: np.ndarray, test_label: pd.Series):\n",
        "  ## <--- LENGKAPI --->\n",
        "  ## Lakukan pelatihan terhadap objek model, kemudian lakukan prediksi terhadap\n",
        "  ## test_df.\n",
        "  None\n",
        "  prediction = None\n",
        "  \n",
        "  print(f\"\"\"\n",
        "  =================================================================\n",
        "  {model_name} \n",
        "  <-------------------------------->\n",
        "  \"\"\")\n",
        "  evaluate_prediction(test_label, prediction)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8LZQWWPVu-w"
      },
      "source": [
        "Terakhir, kita dapat memperluas `evaluate_models` untuk memperumum `evaluate_model` agar bisa menerima banyak model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Fz-Wrdb_S4"
      },
      "source": [
        "def evaluate_models(models: [object], train_df: np.ndarray, train_label: pd.Series, test_df: np.ndarray, test_label: pd.Series):\n",
        "  for model_name, model in models:\n",
        "    evaluate_model(model_name, model, train_df, train_label, test_df, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCreoo-GfIBA"
      },
      "source": [
        "models = [\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('K-Nearest Neighbours', KNeighborsClassifier()),\n",
        "    ('Decision Tree', DecisionTreeClassifier()),\n",
        "]\n",
        "\n",
        "evaluate_models(models, bm_train_df, bm_train_label, bm_val_df, bm_val_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSLOOgtPLQPT"
      },
      "source": [
        "### Performa model setelah *preprocessing* dan rekayasa fitur\n",
        "Mari kita lihat apakah performa model kita bisa lebih baik setelah kita lakukan rekayasa fitur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R-fcisb9Xg3"
      },
      "source": [
        "Umumnya, semua fungsi-fungsi yang berkaitan dengan *preprocessing* dan rekayasa fitur kita kumpulkan dalam satu fungsi besar seperti yang ada di sel berikut. Kumpulan fungsi-fungsi ini lazim disebut *pipeline* untuk data kita sebelum dimodelkan.\n",
        "\n",
        "Untuk sesi ini, semua tahap *preprocessing* dan rekayasa fitur kita akan dikumpulkan pada fungsi `get_data_and_label()` dan `preprocess_data()`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGiwfabgEe7K"
      },
      "source": [
        "Pertama, kita buat fungsi `get_data_and_label()` untuk:\n",
        "1. membuang kolom,\n",
        "1. menyederhanakan fitur kategorikal,\n",
        "1. memetakan label ke 0/1,\n",
        "1. melakukan binning,\n",
        "1. memisahkan fitur dan label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGT_A0VSEgIC"
      },
      "source": [
        "def get_data_and_label(\n",
        "    df: DataFrame,\n",
        "    dropped_cols: [str],\n",
        "    col_map_pairs: [str],\n",
        "    col_bin_pairs: [(str, [float])],\n",
        "    label_col: str,\n",
        "    positive_label: str,\n",
        "    negative_label: str):\n",
        "  \n",
        "  ## <--- LENGKAPI --->\n",
        "  ## Lengkapi fungsi get_data_and_label(). Petunjuk fungsi apa yang harus Anda\n",
        "  ## gunakan ada pada nama variabel. Perhatikan juga langkah-langkah yang \n",
        "  ## ditulis di atas.\n",
        "\n",
        "  result_df = df.copy()\n",
        "  result_df_dropped = None\n",
        "  result_df_mapped_cat = None\n",
        "  result_df_mapped_label = None\n",
        "  result_df_binned = None\n",
        "  data, label = None\n",
        "\n",
        "  return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUQAusgYFCpL"
      },
      "source": [
        "Lalu kita manfaatkan fungsi tersebut untuk melakukan *preprocessing*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxyhOq2O6tvY"
      },
      "source": [
        "def preprocess_data(\n",
        "    df: DataFrame,\n",
        "    dropped_cols: [str],\n",
        "    col_map_pairs: [(str, dict)],\n",
        "    col_bin_pairs: [(str, [float])],\n",
        "    label_col: str,\n",
        "    positive_label: str,\n",
        "    negative_label: str,\n",
        "    column_transformer: ColumnTransformer):\n",
        "  \n",
        "  result_df = df.copy()\n",
        "\n",
        "  ## <--- LENGKAPI --->\n",
        "  ## Dapatkan dataframe yang telah di-preprocess dan dipisah fitur dan labelnya,\n",
        "  ## kemudian gunakan column_transformer untuk mendapatkan data_final.\n",
        "\n",
        "  data, label = None\n",
        "  data_final = None\n",
        "  \n",
        "  return data_final, label\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDs9O5UmdupH"
      },
      "source": [
        "Pertama, kita gunakan `get_data_and_label` untuk memroses `train_df`, kemudian kita akan membuat `ColumnTransformer` untuk melakukan standardisasi dan *one-hot encoding* untuk data latih."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abA2sHGZSY6p"
      },
      "source": [
        "train_raw, train_label = get_data_and_label(train_df, dropped_cols, col_map_pairs, col_bin_pairs, label_col, '>50K', '<=50K')\n",
        "\n",
        "column_transformer = ColumnTransformer(\n",
        "    [\n",
        "      ('standard-scaling-1', StandardScaler(), standardized_cols),\n",
        "      ('ohe-final-1', OneHotEncoder(drop='first', sparse=False), categorical_columns)\n",
        "    ],\n",
        "    remainder='passthrough',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "column_transformer.fit(train_raw)\n",
        "\n",
        "train_df_final = column_transformer.transform(train_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Nh2rd1GaYZ"
      },
      "source": [
        "Kemudian, kita gunakan `preprocess_data()` dan `column_transformer` untuk memroses `val_df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1AqSRE-K5Jj"
      },
      "source": [
        "val_df_final, val_label = preprocess_data(val_df, dropped_cols, col_map_pairs, col_bin_pairs, label_col, '>50K', '<=50K', column_transformer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs3i2k9ce_e6"
      },
      "source": [
        "Kita dapat kembali menggunakan `evaluate_models` untuk menguji model-model yang ingin kita buat terhadap data yang telah kita rekayasa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru1XmqGF5Uqd"
      },
      "source": [
        "evaluate_models(models, prep_train_df_final, train_label, prep_val_df, val_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kjs0k6D-PdY"
      },
      "source": [
        "Jauh lebih baik! Sekarang kita akan masuk pada tahap selanjutnya, yakni *hyperparameter tuning*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9hYm0x8_V5V"
      },
      "source": [
        "## Bagian 4 : *Hyperparameter tuning*\n",
        "\n",
        "Dalam pembelajaran mesin, biasanya kita melakukan suatu proses optimisasi sehingga kita memperoleh parameter model yang memiliki galat minimum. Akan tetapi, untuk beberapa model, biasanya ada asumsi tertentu yang kita tetapkan sebelum memulai proses optimisasi. Pada model berbasis pohon, misalnya, kita menetapkan maksimal kedalaman pohon, atau pada model linear, kita menetapkan besarnya koefisien regularisasi. Asumsi-asumsi ini kita sebut sebagai *hyperparameter*.  \n",
        "\n",
        "Tentu ada risiko bahwa kita tidak menemukan model yang terbaik karena kita salah mengambil asumsi tentang *hyperparameter* model kita. Untuk itu, kita melakukan proses *tuning*, yakni mencari *hyperparameter* yang akan memberikan hasil terbaik untuk model kita. Ada beberapa cara untuk melakukan ini.  Cara yang paling mudah adalah teknik *grid search*: mencoba beberapa kombinasi *hyperparameter* yang mungkin.\n",
        "\n",
        "Berikut adalah contoh untuk regresi logistik:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcxf_KwQpDz_"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = LogisticRegression()\n",
        "params = {'C': [0.01, 0.05, 0.1, 1.0, 10.0]}\n",
        "lr_with_hyperparam_sets = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
        "evaluate_model(\"Logistic regression hyperpameter tuning\", lr_with_hyperparam_sets, train_df_final, train_label, val_df_final, val_label)\n",
        "\n",
        "print('Best hyperparameters:\\n', lr_with_hyperparam_sets.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjXvPBVTxaC"
      },
      "source": [
        "Dan berikut adalah contoh untuk *decision tree*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnL3nQnnT1ig"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Buat sebuah instance objek DecisionTreeClassifier tanpa argumen constructor.\n",
        "dt_model = None\n",
        "\n",
        "## Definisikan parameter grid search untuk GridSearchCV. Kita akan menggunakan:\n",
        "## criterion: gini, entropy\n",
        "## max_depth: 5, 10, 15, 20, 25, 30, 35, 40\n",
        "## min_impurity_decrease: 0.0, 0.01, 0.05, 0.1, 0.25, 0.5\n",
        "\n",
        "dt_params = None\n",
        "\n",
        "## Buat objek GridSearchCV dengan model dt_model, param_grid dt_params, dan \n",
        "## n_jobs = -1.\n",
        "dt_with_hyperparam_sets = None\n",
        "evaluate_model(\"Decision tree hyperpameter tuning\", dt_with_hyperparam_sets, train_df_final, train_label, val_df_final, val_label)\n",
        "\n",
        "## Cetak hyperparameter terbaik dt_with_hyperparam_sets.\n",
        "print('Best hyperparameters:\\n', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDF11fypFCa"
      },
      "source": [
        "## Bagian 5: Evaluasi model terhadap data uji\n",
        "\n",
        "Setelah bersusah payah membuat model, tentu kita ingin tahu bagaimana kemampuan model kita membuat prediksi untuk data yang tidak pernah dilihat sebelumnya. Inilah saatnya kita menggunakan data uji."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV31TxEBpU-Q"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Lakukan preprocessing terhadap test_df.\n",
        "\n",
        "test_df_final, test_label = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbeBPSDBzeiH"
      },
      "source": [
        "print(\"Number of rows: {}\".format(len(test_df_final)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZPNRMmwpmfd"
      },
      "source": [
        "prep_test_df, test_label = separate_data_and_label(prep_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0NTpL9rWIP6"
      },
      "source": [
        "Mari kita lihat performa model regresi logistik yang telah di-*tune* *hyperparameter*-nya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC0E72pkpyLC"
      },
      "source": [
        "## <--- LENGKAPI --->\n",
        "## Buatlah sebuah objek LogisticRegression yang baru menggunakan hyperparameter\n",
        "## terbaik yang telah dihitung.\n",
        "## Anda bisa memasukkan nilai-nilainya secara langsung, atau gunakan sintaks\n",
        "## **dict (misalnya LogisticRegression(**params)) untuk secara otomatis \n",
        "## menggunakan sebuah dictionary sebagai ganti argumen ber-keyword.\n",
        "##\n",
        "## Setelah Anda membuat objek model, latih model tersebut dan lakukan prediksi\n",
        "## terhadap prep_test_df.\n",
        "\n",
        "model = None\n",
        "None\n",
        "\n",
        "test_prediction = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EakCeYWLqQqS"
      },
      "source": [
        "evaluate_prediction(test_label, test_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0GsTIkYiBC9"
      },
      "source": [
        "Untuk model-model yang lain, caranya pun serupa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HjksGwot7F9"
      },
      "source": [
        "models = [\n",
        "    ('K-Nearest Neighbours', KNeighborsClassifier()),\n",
        "    ('Decision Tree', DecisionTreeClassifier()),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uidMl7lYiVag"
      },
      "source": [
        "evaluate_models(models, prep_train_df_final, train_label, prep_test_df, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}